{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455bc77a-b26a-4525-bdb6-3e219290951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-1\n",
    "Probability Mass Function (PMF):\n",
    "The PMF is used for discrete random variables. It assigns probabilities to each possible value that the random variable can take. In other words, it gives the probability of observing a specific outcome. The PMF is defined as:\n",
    "PMF(x) = P(X = x)\n",
    "\n",
    "where X is the random variable and x is a specific value that X can take. The PMF satisfies the following properties:\n",
    "\n",
    "The value of PMF(x) is always non-negative.\n",
    "The sum of PMF(x) over all possible values of x is equal to 1.\n",
    "Example:\n",
    "Consider a fair six-sided die. The random variable X represents the outcome of rolling the die. The PMF for this example would be:\n",
    "PMF(1) = 1/6\n",
    "PMF(2) = 1/6\n",
    "PMF(3) = 1/6\n",
    "PMF(4) = 1/6\n",
    "\n",
    "Probability Density Function (PDF):\n",
    "The PDF is used for continuous random variables. It represents the probability distribution of the variable over a range of values. Unlike the PMF, which assigns probabilities to specific values, the PDF gives the relative likelihood of observing a value within a given range. The PDF is defined as:\n",
    "PDF(x) = f(x)\n",
    "\n",
    "where f(x) is the derivative of the cumulative distribution function (CDF) of the random variable.\n",
    "\n",
    "Example:\n",
    "Consider a continuous random variable X that follows a standard normal distribution (mean = 0, standard deviation = 1). The PDF for this example is given by the bell-shaped curve known as the standard normal distribution. The PDF represents the relative likelihood of observing a value of X within a range. However, since the standard normal distribution is continuous, the probability of observing a specific value is zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3691307-ec9d-4c59-81fb-630c6fde1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-2\n",
    "The Cumulative Density Function (CDF) is a function used in probability theory and statistics to describe the cumulative probability distribution of a random variable. It gives the probability that the random variable takes on a value less than or equal to a specified value.\n",
    "\n",
    "Mathematically, the CDF of a random variable X is defined as:\n",
    "\n",
    "CDF(x) = P(X ≤ x)\n",
    "\n",
    "where x is a specific value of X, and P(X ≤ x) represents the probability that X takes on a value less than or equal to x.\n",
    "\n",
    "The CDF satisfies the following properties:\n",
    "\n",
    "The value of CDF(x) is always between 0 and 1.\n",
    "The CDF is non-decreasing, meaning that as x increases, CDF(x) also increases.\n",
    "The limit of the CDF as x approaches negative infinity is 0, and as x approaches positive infinity is 1.\n",
    "Probability Calculation: The CDF can be used to calculate probabilities associated with a random variable. For example, the probability that X lies between two values a and b can be obtained by subtracting the CDF at a from the CDF at b: P(a < X ≤ b) = CDF(b) - CDF(a).\n",
    "\n",
    "Quantile Calculation: The CDF can be used to find quantiles, which are specific values that divide the probability distribution into equally likely regions. For example, the median is the value at which the CDF is equal to 0.5, dividing the distribution into two equal halves.\n",
    "Example:\n",
    "Let's consider a random variable X representing the heights of adult males in a population. The CDF for this variable would provide the probability that a randomly selected adult male has a height less than or equal to a given value x. For instance, the CDF at x = 180 cm might be 0.75, indicating that 75% of adult males in the population have a height less than or equal to 180 cm.\n",
    "\n",
    "The CDF is useful for analyzing and summarizing the probability distribution of a random variable. It allows for various calculations and comparisons, such as determining percentiles, estimating probabilities, and understanding the overall shape of the distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b3bd4-5068-4301-9055-8842690329b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-3\n",
    "Heights and Weights: The heights and weights of a large population tend to follow a roughly normal distribution, with most people clustering around the average height or weight and fewer individuals at the extremes.\n",
    "\n",
    "IQ Scores: IQ scores are often assumed to be normally distributed, with the majority of the population falling around the average IQ score and fewer individuals having exceptionally high or low scores.\n",
    "\n",
    "Measurement Errors: In many scientific experiments, measurement errors are assumed to be normally distributed. This assumption allows researchers to make statistical inferences and estimate the uncertainty associated with the measurements.\n",
    "\n",
    "Financial Markets: Stock prices and returns in financial markets are often modeled using the normal distribution, especially when analyzing short-term changes. This assumption forms the basis for various financial models and risk assessments.\n",
    "\n",
    "Biological Phenomena: Many biological characteristics, such as enzyme activity, blood pressure, and gene expression levels, can be approximately modeled using the normal distribution.\n",
    "\n",
    "Mean (μ): The mean represents the central tendency or the average value of the distribution. It defines the location of the peak of the bell curve. Shifting the mean to the left or right moves the entire distribution along the x-axis without changing its shape.\n",
    "\n",
    "Standard Deviation (σ): The standard deviation measures the spread or dispersion of the data points around the mean. A smaller standard deviation indicates that the data points are tightly clustered around the mean, resulting in a narrower and taller bell curve. Conversely, a larger standard deviation leads to a broader and flatter curve, indicating a greater dispersion of data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1a371-c540-4182-accb-fc7290bd6b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-4\n",
    "Central Limit Theorem: The normal distribution plays a fundamental role in the Central Limit Theorem (CLT). According to the CLT, the sum or average of a large number of independent and identically distributed random variables tends to follow a normal distribution, regardless of the underlying distribution of the individual variables. This property makes the normal distribution a key tool for analyzing and making inferences about a wide range of real-world phenomena.\n",
    "\n",
    "Statistical Inference: Many statistical methods and techniques are based on the assumption of normality. For example, parametric hypothesis tests, confidence intervals, and regression analysis often assume that the data follow a normal distribution. By assuming normality, these methods can provide accurate and reliable results.\n",
    "\n",
    "Estimation and Prediction: The normal distribution allows for efficient estimation and prediction in many scenarios. With the assumption of normality, maximum likelihood estimation and Bayesian inference techniques can be used to estimate parameters and make predictions with good precision.\n",
    "\n",
    "Standardization and Z-Scores: The normal distribution has well-defined properties that make it easy to standardize and compare different values or observations. By transforming data into standard Z-scores, one can determine how unusual or extreme an observation is within a given dataset.\n",
    "\n",
    "EXAMPLE:\n",
    "\n",
    "Human Characteristics: Height, weight, IQ scores, blood pressure, and various biological measurements tend to exhibit a normal distribution pattern in large populations.\n",
    "\n",
    "Test Scores: Standardized tests, such as SAT or IQ tests, are often designed to have a normal distribution of scores, allowing for comparisons and rankings.\n",
    "\n",
    "Errors and Residuals: Measurement errors, experimental residuals, and noise in many scientific and engineering fields are often assumed to follow a normal distribution.\n",
    "\n",
    "Financial Data: In finance, stock returns, asset prices, and interest rate changes are frequently modeled using the normal distribution or variations like the log-normal distribution.\n",
    "\n",
    "Quality Control: Process measurements, defects in manufacturing, and product characteristics often exhibit a normal distribution pattern in quality control applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8db7b-eac1-426a-a0a5-1e9636d73dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-5\n",
    "The Bernoulli distribution is a discrete probability distribution that models a single Bernoulli trial, which is an experiment with two possible outcomes: success (usually denoted as 1) or failure (usually denoted as 0). It is named after Jacob Bernoulli, a Swiss mathematician.\n",
    "\n",
    "The probability mass function (PMF) of the Bernoulli distribution is given by:\n",
    "\n",
    "P(X = x) = p^x * (1-p)^(1-x)\n",
    "\n",
    "where X is a random variable that takes values 0 or 1, and p is the probability of success.\n",
    "\n",
    "Example:\n",
    "Consider a coin toss experiment, where success is defined as obtaining a \"heads\" outcome, and failure is obtaining a \"tails\" outcome. In this case, the Bernoulli distribution can be used to model the probability of getting a \"heads\" (success) on a single coin toss. Let's say the probability of getting a \"heads\" is 0.6 (p = 0.6), then the Bernoulli distribution would be:\n",
    "\n",
    "P(X = 0) = (1-0.6)^1 = 0.4\n",
    "P(X = 1) = 0.6^1 = 0.6\n",
    "The Bernoulli distribution in this example tells us the probability of success (getting a \"heads\") or failure (getting a \"tails\") in a single coin toss.\n",
    "\n",
    "Difference between Bernoulli Distribution and Binomial Distribution:\n",
    "\n",
    "Number of Trials:\n",
    "Bernoulli Distribution: Represents a single Bernoulli trial, which has only two possible outcomes (success or failure).\n",
    "Binomial Distribution: Represents the number of successes in a fixed number of independent Bernoulli trials.\n",
    "Number of Parameters:\n",
    "Bernoulli Distribution: Has a single parameter, p, representing the probability of success in a single trial.\n",
    "Binomial Distribution: Has two parameters, n and p, where n represents the number of trials and p represents the probability of success in each trial.\n",
    "Random Variable:\n",
    "Bernoulli Distribution: The random variable in the Bernoulli distribution takes values 0 and 1, representing failure and success, respectively.\n",
    "Binomial Distribution: The random variable in the binomial distribution represents the number of successes in a fixed number of trials, and it can take on values from 0 to n, where n is the number of trials.\n",
    "Probability Mass Function (PMF):\n",
    "Bernoulli Distribution: The PMF of the Bernoulli distribution is given by P(X = x) = p^x * (1-p)^(1-x), where x can be either 0 or 1.\n",
    "Binomial Distribution: The PMF of the binomial distribution is given by P(X = k) = C(n, k) * p^k * (1-p)^(n-k), where k represents the number of successes, n is the number of trials, p is the probability of success in each trial, and C(n, k) is the binomial coefficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd596f9-b77f-43b8-b3f6-4b408e43b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-6\n",
    "z = (x - μ) / σ\n",
    "z = (60 - 50) / 10 = 1\n",
    "P(X > 60) = 1 - P(Z ≤ 1)\n",
    "P(X > 60) = 1 - 0.8413 = 0.1587\n",
    " the probability is approximately 0.1587 or 15.87%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaaed5b-4564-4666-9695-e013214b8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-7\n",
    "The uniform distribution is a continuous probability distribution that is characterized by a constant probability density function (PDF) over a defined interval. It is called \"uniform\" because the probability is evenly distributed across the interval, resulting in a rectangular-shaped distribution.\n",
    "\n",
    "In a uniform distribution, every value within the interval has an equal likelihood of occurring. The PDF of the uniform distribution is given by:\n",
    "\n",
    "f(x) = 1 / (b - a)\n",
    "\n",
    "where 'a' and 'b' are the lower and upper bounds of the interval, respectively.\n",
    "\n",
    "Example:\n",
    "Consider a fair six-sided die. Each face of the die has an equal probability of showing up, so we can model the outcome of rolling the die as a uniform distribution.\n",
    "In this example, the interval is defined as [1, 6], representing the possible values on the die. Since it is a fair die, each value has an equal probability of 1/6.\n",
    "\n",
    "The PDF for this uniform distribution is:\n",
    "\n",
    "f(x) = 1 / (6 - 1) = 1/5\n",
    "This means that for any value x within the interval [1, 6], the probability of observing that value is 1/5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a056f8-1b14-48c9-9455-db957472a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-8\n",
    "The z-score, also known as the standard score, is a measure that quantifies how many standard deviations an individual data point or observation is away from the mean of a dataset. It is a standardized value that allows for comparisons and analysis across different datasets with varying means and standard deviations.\n",
    "\n",
    "The z-score of a data point x is calculated using the formula:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "where x is the data point, μ is the mean of the dataset, and σ is the standard deviation of the dataset.\n",
    "\n",
    "The importance of the z-score lies in its ability to provide insights about the relative position or rarity of a data point within a distribution. It allows us to:\n",
    "\n",
    "Standardize Data: By converting data points into z-scores, we can standardize and transform them to a common scale. This enables meaningful comparisons and analysis between different datasets with different means and standard deviations.\n",
    "Assess Relative Position: The z-score indicates whether a data point is above or below the mean and by how many standard deviations. Positive z-scores represent data points above the mean, while negative z-scores represent data points below the mean. A higher (positive) z-score indicates a greater deviation from the mean.\n",
    "\n",
    "Identify Outliers: Unusually large or small z-scores can be used to identify outliers, which are data points that deviate significantly from the bulk of the dataset. Outliers may indicate potential errors, anomalies, or interesting observations that warrant further investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23690bda-a0a2-4028-b67d-b01833c69a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-9\n",
    "The Central Limit Theorem (CLT) is a fundamental concept in probability theory and statistics. It states that when independent random variables are added together, regardless of their individual distributions, the sum or average tends to follow a normal distribution as the number of variables increases.\n",
    "\n",
    "The Central Limit Theorem can be stated as follows:\n",
    "\n",
    "Given a random sample of n independent and identically distributed (i.i.d.) random variables X₁, X₂, ..., Xₙ, with a finite mean (μ) and standard deviation (σ), as n approaches infinity, the distribution of the sample mean (or sum) approaches a normal distribution with a mean of μ and a standard deviation of σ/√n.\n",
    "\n",
    "Significance of the Central Limit Theorem:\n",
    "\n",
    "Approximation of Distributions: The Central Limit Theorem allows us to approximate the distribution of the sample mean (or sum) to a normal distribution, even if the underlying population distribution is not normal. This is especially useful when the sample size is large. It provides a practical tool for making inferences and conducting hypothesis testing.\n",
    "Statistical Inference: The Central Limit Theorem is the foundation for many statistical methods, such as confidence intervals and hypothesis testing. It enables us to make accurate inferences about population parameters based on sample statistics.\n",
    "\n",
    "Sampling Theory: The Central Limit Theorem justifies the use of sampling techniques and the reliance on the sample mean as an estimator of the population mean. It allows us to draw conclusions about the population based on a smaller representative sample.\n",
    "\n",
    "Stability and Robustness: The Central Limit Theorem demonstrates the stability and robustness of the normal distribution. It shows that even if the individual variables have different distributions, as long as they are independent and have finite means and variances, their sum or average tends to converge to a normal distribution. This property is widely applicable in various fields of research and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf48b3b-c0dc-4f15-90e1-7cd8e3237e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-10\n",
    "The Central Limit Theorem (CLT) relies on certain assumptions in order to hold true. These assumptions include:\n",
    "\n",
    "Independent and Identically Distributed (i.i.d.) Variables: The random variables included in the sample must be independent of each other, meaning that the value of one variable does not affect the value of another. Additionally, the variables should be identically distributed, meaning they have the same probability distribution.\n",
    "\n",
    "Finite Mean and Variance: The random variables should have finite means (μ) and variances (σ^2). This assumption ensures that the average and sum of the variables are well-defined.\n",
    "\n",
    "Sample Size: As the sample size (n) increases, the approximation to a normal distribution becomes more accurate. The CLT assumes that the sample size is sufficiently large for the approximation to hold. While there is no fixed rule for the minimum sample size, a commonly suggested guideline is that the sample size should be at least 30.\n",
    "It is important to note that violating these assumptions may lead to the Central Limit Theorem not holding true. In such cases, alternative methods or distributions might be more appropriate for analysis.\n",
    "\n",
    "Additionally, it is worth mentioning that there are variations of the Central Limit Theorem that relax some of these assumptions, such as the Lindeberg-Levy CLT and the Lyapunov CLT, which accommodate certain types of dependence or relax the finite variance requirement.\n",
    "\n",
    "Therefore, when applying the Central Limit Theorem, it is crucial to consider whether the assumptions are satisfied in the specific context of the data and analysis being conducted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
